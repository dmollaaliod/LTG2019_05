{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Comparing Minimum Risk Training and REINFORCE\n",
    "## Diego Molla-Aliod\n",
    "LTG, 6 May 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "In this talk I show some notes and reflections between Minimum Risk Training and REINFORCE, based on the following papers:\n",
    "\n",
    "- Ayana, Shen, S., Zhao, Y., Liu, Z., & Sun, M. (2016). Neural Headline Generation with Sentence-wise Optimization. Retrieved from http://arxiv.org/abs/1604.01904\n",
    "- Shen, S., Cheng, Y., He, Z., He, W., Wu, H., Sun, M., & Liu, Y. (2015). Minimum Risk Training for Neural Machine Translation. Retrieved from http://arxiv.org/abs/1512.02433\n",
    "- Williams, R. J. (1992). Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning. Machine Learning, 8, 229â€“256. Retrieved from http://www-anw.cs.umass.edu/~barto/courses/cs687/williams92simple.pdf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Minimum Risk Training (Shen et al, 2015)\n",
    "MRT has the following advantages over MLE:\n",
    "1. **Direct optimization with respect to evaluation metrics**: MRT introduces evaluation metrics as loss functions and aims to minimize expected loss on the training data.\n",
    "2. **Applicable to arbitrary loss functions**, which are not necessarily differentiable.\n",
    "3. **Transparent to architectures**: MRT does not assume the specific architectures of NMT and can be applied to ant end-to-end NMT systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# End-to-end NMT: Using MLE\n",
    "\n",
    "\n",
    "![NMT-background](images/NMT-background.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![NMT-background2](images/NMT-background2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![NMT-background3](images/NMT-background3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Problems with MLE\n",
    "\n",
    "1. **Exposure bias**: While the models are trained only on the training data distribution, they are used to generate target words on previous model predictions, which can be erroneous, at test time.\n",
    "2. **Loss function**: MLE usually uses the cross-entropy loss focusing on word-level errors to maximize the probability of the next correct word, which might hardly correlate well with corpus-level and sentence-level evaluation metrics such as BLEU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Enters MRT\n",
    "\n",
    "In MRT, the *risk* is defined as the expected loss with respect to the posterior distribution:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "{\\cal R}(\\theta) & = \\sum_{s=1}^S \\mathbb{E}_{y|x^{(s)};\\theta}\\left[ \\Delta(y,y^{(s)})\\right]\\\\\n",
    "& = \\sum_{s=1}^S \\sum_{y\\in {\\cal Y}(x^{(s)})}P(y|x^{(s)};\\theta) \\Delta(y,y^{(s)})\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "where ${\\cal Y}(x^{(s)})$ is a set of all possible translations for $x^{(s)}$ and $\\Delta(y,y^{(s)})$ is a loss function. **The loss function is not parameterized and thus it does not need to be differentiable.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The training objective of MRT is to minimise the risk on the training data:\n",
    "\n",
    "$$\\hat{\\theta}_{\\hbox{MRT}} = \\arg\\min_\\theta\\left\\{{\\cal R}(\\theta)\\right\\}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "In MRT, the partial derivative with respect to a model parameter $\\theta_i$ is given by:\n",
    "![MRT-derivative](images/MRT-derivative.png)\n",
    "* Since Eq. (10) suggests there is no need to differentiate $\\Delta(y,y^{(s)})$, MRT allows arbitrary non-differentiable loss functions.\n",
    "* In addition, the approach is transparent to architectures and can be applied to arbitrary end-to-end NMT models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Solving MRT\n",
    "However, $\\mathbb{E}_{y|x^{(s)};\\theta}\\left[ \\Delta(y,y^{(s)})\\right]$ is usually intractable to calculate due to:\n",
    "1. the exponential search space of $\\cal Y(x^{(s)})$,\n",
    "2. the non-decomposability of the loss function $\\Delta(y,y^{(s)})$, and\n",
    "3. the context sensitiveness of NMT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "To alleviate this problem, Shen et al. (2015) propose to **use a subset of the full search space to approximate the posterior distribution** and introduce a new training objective:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\tilde{\\cal R}(\\theta) & = \\sum_{s=1}^S \\mathbb{E}_{y|x^{(s)};\\theta,\\alpha}\\left[ \\Delta(y,y^{(s)})\\right]\\\\\n",
    "& = \\sum_{s=1}^S \\sum_{y\\in S(x^{(s)})}Q(y|x^{(s)};\\theta,\\alpha) \\Delta(y,y^{(s)})\\\\\n",
    "Q(y|x^{(s)};\\theta,\\alpha) & = \\frac{P(y|x^{(s)};\\theta)^\\alpha}{\\sum_{y'\\in S(x^{(s)})}P(y'|x^{(s)};\\theta)^\\alpha}\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "1. $S(x^{(s)}) \\subset \\cal Y(x^{(s)})$ is a sampled subset of the search space (see Algorithm 1),\n",
    "2. $Q(y|x^{(s)};\\theta)^\\alpha)$ is a distribution defined on the subspace $S(x^{(s)})$, and\n",
    "3. $\\alpha$ is a hyper-parameter that controls the sharpness of the $Q$ distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![Sampling](images/Sampling.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Given the sampled space, the partial derivative with respect to a model parameter $\\theta_i$ of $\\tilde{\\cal R}(\\theta)$ is given by\n",
    "![MRT-derivative2](images/MRT-derivative2.png)\n",
    "Since $|S(x^{(s)}|$ can be relatively small (in their experiments, 100 was good enough), the expectations in Eq.(14) can be efficiently calculated by explicitly enumerating all candidates in $S(x^{((s)})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "But also:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial \\tilde{\\cal R}(\\theta)}{\\partial\\theta_i} & = \\frac{\\partial}{\\partial\\theta_i}\\left[\\sum_{s=1}^S\\sum_{y\\in S(x^{(s)})}Q(y|x^{(s)};\\theta,\\alpha)\\Delta(y,y^{(s)})\\right]\\\\\n",
    "& = \\sum_{s=1}^S\\sum_{y\\in S(x^{(s)})}\\frac{\\partial Q(y|x^{(s)};\\theta,\\alpha)}{\\partial\\theta_i}\\Delta(y,y^{(s)})\\\\\n",
    "Q(y|x^{(s)};\\theta,\\alpha) & = \\frac{P(y|x^{(s)};\\theta)^\\alpha}{\\sum_{y'\\in S(x^{(s)})}P(y'|x^{(s)};\\theta)^\\alpha}\\\\\n",
    "\\end{align}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# REINFORCE\n",
    "\n",
    "- REINFORCE is a policy gradient approach to reinforcement learning.\n",
    "- aka Monte Carlo policy differentiation.\n",
    "- The agent takes an action given a state so that the expected reward $r(s,a)$ is maximum.\n",
    "- A stochastic *policy* $\\pi_\\theta(a|s)$ is a function that has parameters $\\theta$ and determines the probability of the action $a$ to take given the state $s$.\n",
    "![reinforcement-learning](images/Reinforcement_learning_diagram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The following material is based on:\n",
    "- https://mcneela.github.io/math/2018/04/18/A-Tutorial-on-the-REINFORCE-Algorithm.html\n",
    "- https://lilianweng.github.io/lil-log/2018/04/08/policy-gradient-algorithms.html (?? this one too??)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The objective function $J$ returns the expected reward achieved by a given policy $\\pi_{\\theta}$ over some time horizon governed by $t$ (which can be either finite or infinite):\n",
    "$$J(\\theta) = \\mathbb{E}_{\\tau \\sim p_\\theta(\\tau)} \\left[ \\sum_t r(s_t, a_t) \\right]$$\n",
    "Where $\\tau \\sim p_\\theta(\\tau)$ indicates that we're sampling trajectories $\\tau$ from the probability distribution of our policy approximator governed by $\\theta$.\n",
    "$$p_\\theta(\\tau) = p_\\theta(s_1, a_1, \\ldots, s_T, a_T) = p(s_1) \\prod_{t=1}^T \\pi_\\theta(a_t \\mid s_t) p(s_{t+1} \\mid s_t, a_t)$$\n",
    "We can now specify the optimal policy $\\pi^* = \\pi_{\\theta^*} =\n",
    "arg\\,max_\\theta J(\\theta)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The **Monte-Carlo Approximation** is based on this following fact:\n",
    "$$\\lim_{N \\to \\infty} \\frac{1}{N}\\sum_{i=1}^N f(x_i)_{x_i \\sim p(x)} = \\mathbb{E}[f(x)]$$\n",
    "Thus, we rewrite our objective function as:\n",
    "$$J(\\theta) \\approx \\frac{1}{N} \\sum_{i=1}^N \\sum_{t} r(s_{i, t}, a_{i, t})$$\n",
    "Where the $N$ samples are being directly drawn from the probability distribution defined by $\\pi_\\theta$ simply by running $\\pi_\\theta$, $N$ times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "It can be shown that the gradient of the loss can be computed as:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\nabla_\\theta J(\\theta) & = \\mathbb{E}_{\\tau \\sim \\pi_\\theta(\\tau)}\n",
    "\\left[ \\left(\\sum_{t} \\nabla_\\theta \\log{\\pi_\\theta}(a_t \\mid s_t)\\right) \\left(\\sum_t r(s_t, a_t)\\right)\\right]\\\\\n",
    "& \\approx \\frac{1}{N} \\sum_{i=1}^N \\left[ \\left(\\sum_{t} \\nabla_\\theta \\log{\\pi_\\theta}(a_{i, t} \\mid s_{i,t})\\right) \\left(\\sum_t r(s_{i,t}, a_{i,t})\\right)\\right]\n",
    "\\end{align}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The REINFORCE algorithm uses the above to learn the parameters of the loss:\n",
    "1. Sample trajectories $\\left\\{\\tau_i\\right\\}_{i=1}^N$ from $\\pi_\\theta(a_t|s_t)$ by running the policy.\n",
    "2. Set $\\nabla_\\theta J(\\theta) = \\sum_i \\left(\\sum_t \\nabla_\\theta \\log \\pi_\\theta(a^i_t \\mid s^i_t)\\right) \\left(\\sum_t r(s^i_t, a^i_t)\\right)$\n",
    "3. $\\theta \\leftarrow \\theta + \\alpha \\nabla_\\theta J(\\theta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can therefore see that MRT and REINFORCE are based on the same loss function. This may have implications on improving systems that use MRT, such as NMT and summarisation systems. In particular, consider using other Policy gradient methods to calculate MRT, such as Actor-Critic, DPG, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "IPython (Python 3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
