{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Comparing Minimum Risk Training and REINFORCE\n",
    "## Diego Molla-Aliod\n",
    "LTG, 6 May 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "In this talk I show some notes and reflections between Minimum Risk Training and REINFORCE, based on the following papers:\n",
    "\n",
    "- Ayana, Shen, S., Zhao, Y., Liu, Z., & Sun, M. (2016). Neural Headline Generation with Sentence-wise Optimization. Retrieved from http://arxiv.org/abs/1604.01904\n",
    "- Shen, S., Cheng, Y., He, Z., He, W., Wu, H., Sun, M., & Liu, Y. (2015). Minimum Risk Training for Neural Machine Translation. Retrieved from http://arxiv.org/abs/1512.02433\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Minimum Risk Training (Shen et al, 2015)\n",
    "MRT has the following advantages over MLE:\n",
    "1. **Direct optimization with respect to evaluation metrics**: MRT introduces evaluation metrics as loss functions and aims to minimize expected loss on the training data.\n",
    "2. **Applicable to arbitrary loss functions**, which are not necessarily differentiable.\n",
    "3. **Transparent to architectures**: MRT does not assume the specific architectures of NMT and can be applied to ant end-to-end NMT systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# End-to-end NMT: Using MLE\n",
    "\n",
    "\n",
    "![NMT-background](images/NMT-background.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![NMT-background2](images/NMT-background2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![NMT-background3](images/NMT-background3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Problems with MLE\n",
    "\n",
    "1. **Exposure bias**: While the models are trained only on the training data distribution, they are used to generate target words on previous model predictions, which can be erroneous, at test time.\n",
    "2. **Loss function**: MLE usually uses the cross-entropy loss focusing on word-level errors to maximize the probability of the next correct word, which might hardly correlate well with corpus-level and sentence-level evaluation metrics such as BLEU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Enters MRT\n",
    "\n",
    "In MRT, the *risk* is defined as the expected loss with respect to the posterior distribution:\n",
    "\n",
    "![MRT-risk](images/MRT-risk.png)\n",
    "\n",
    "where $\\cal Y(x^{(s)})$ is a set of all possible translations for $x^{(s)}$ and $\\Delta(y,y^{(s)})$ is a loss function. **The loss function is not parameterized and thus it does not need to be differentiable.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The training objective of MRT is to minimise the risk on the training data:\n",
    "![MRT-objective](images/MRT-objective.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In MRT, the partial derivative with respect to a model parameter $\\theta_i$ is given by:\n",
    "![MRT-derivative](images/MRT-derivative.png)\n",
    "* Since Eq. (10) suggests there is no need to differentiate $\\Delta(y,y^{(s)})$, MRT allows arbitrary non-differentiable loss functions.\n",
    "* In addition, the approach is transparent to architectures and can be applied to arbitrary end-to-end NMT models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Solving MRT\n",
    "However, the expectations in Eq. (10) are usually intractable to calculate due to:\n",
    "1. the exponential search space of $\\cal Y(x^{(s)})$,\n",
    "2. the non-decomposability of the loss function $\\Delta(y,y^{(s)})$, and\n",
    "3. the context sensitiveness of NMT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "To alleviate this problem, Shen et al. (2015) propose to **use a subset of the full search space to approximate the posterior distribution** and introduce a new training objective:\n",
    "![MRT-risk-estimate](images/MRT-risk-estimate.png)![MRT-risk-estimate2](images/MRT-risk-estimate2.png) \n",
    "Where:\n",
    "1. $S(x^{(s)}) \\subset \\cal Y(x^{(s)})$ is a sampled subset of the search space (see Algorithm 1),\n",
    "2. $Q(y|x^{(s)};\\theta)^\\alpha)$ is a distribution defined on the subspace $S(x^{(s)})$, and\n",
    "3. $\\alpha$ is a hyper-parameter that controls the sharpness of the $Q$ distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![Sampling](images/Sampling.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Given the sampled space, the partial derivative with respect to a model parameter $\\theta_i$ of $\\tilde{\\cal R}(\\theta)$ is given by\n",
    "![MRT-derivative2](images/MRT-derivative2.png)\n",
    "Since $|S(x^{(s)}|$ can be relatively small (in their experiments, 100 was good enough), the expectations in Eq.(14) can be efficiently calculated by explicitly enumerating all candidates in $S(x^{((s)})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# REINFORCE"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
